{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS 583 Assignment 3: Semi-supervised Flower Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷 M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm XXX, XXXXXXXXXX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised Flower Classfication\n",
    "\n",
    "In this approach, you have a dataset that includes both labeled and unlabeled examples.\n",
    "\n",
    "The goal is to use the labeled data to train the model while also leveraging the unlabeled\n",
    "data to improve the model's performance.\n",
    "\n",
    "In this assignment, you’ll explore a self-training mechanism for this task.\n",
    "\n",
    "\n",
    "**Please note that you’re not allowed to use pre-constructed models or pre-trained weights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/e304bb12c8a84e5c9c1b27a6c3bd4026) to join the competition.\n",
    "\n",
    "**Again, Use your SID as your team's name!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.\n",
    "\n",
    "We use `python==3.10.12`, `torch==2.0.1+cu118` and `torchvision==0.15.2+cu118`. This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [Flowers Recognition](https://www.kaggle.com/alxmamaev/flowers-recognition) dataset.\n",
    "This is collected by Alexander Mamaev.\n",
    "\n",
    "**Abstrct**  \n",
    "\n",
    "We clean the dataset,this dataset contains 4262 flower images.   \n",
    "**IMPORTANT: you CANNOT use any extra images.**\n",
    "\n",
    "The data collection is grabed from the data flicr, google images, yandex images.\n",
    "You can use this datastet to recognize plants from the photo.\n",
    "\n",
    "The pictures are divided into five classes: \n",
    "+ daisy\n",
    "+ tulip\n",
    "+ rose\n",
    "+ sunflower\n",
    "+ dandelion\n",
    "\n",
    "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Get Data\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `A3_data_flower_2023.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/1eme754s_uI5dI5SnNUH2ZuvJ5QT-kmaZ/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip `A3_data_flower_2023.zip`, there are 2 folders and 4 csvs.\n",
    "\n",
    "- `train/`: contains 5 folders for 5 categories of flowers. Images of flowers inside them.\n",
    "- `test/`: unclassified images of testing set.\n",
    "- `train_labeled_dataset.csv`: file path and true label of training set.\n",
    "- `train_unlabeled_dataset.csv`: file path and without label of training set.\n",
    "- `val.csv`: file path and true label of validation set.\n",
    "- `test.csv`: file paht of testing set.\n",
    "\n",
    "There are **843 images in labeled_dataset_train.**  \n",
    "\n",
    "There are **1713 images in unlabeled_dataset_train.** \n",
    "\n",
    "There are **853 images in dataset_test.**  \n",
    "\n",
    "There are **853 images in dataset_val.**  \n",
    "\n",
    "---\n",
    "\n",
    "解壓縮 `A3_data_flower_2023.zip` 後可以發現裡面有兩個資料夾和四個csv檔。\n",
    "\n",
    "+ `train` : 存有五個資料夾分別是五個種類的花，資料夾內為花的照片。\n",
    "+ `test` : 資料夾中為未分類之測試集照片。\n",
    "+ `train_labeled_dataset.csv` : 讀取 train data 的順序、路徑與圖片所屬花別。\n",
    "+ `train_unlabeled_dataset.csv` : 讀取 train data 的順序、路徑與圖片但沒有所屬花別標籤。\n",
    "+ `val.csv` : 讀取 validate data 的順序、路徑與圖片所屬花別。\n",
    "+ `test.csv` : 讀取 test data 的順序、路徑。\n",
    "\n",
    "其中`train_labeled`的圖片 843 張，`train_unlabeled`的圖片 1713 張，`val` 的圖片 853 張，`test` 的圖片 853 張。\n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq ./drive/MyDrive/A3_data_flower_2023.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'A3_data_flower_2023'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset\n",
    "\n",
    "Build a classs inherit `torch.utils.data.Dataset`.  \n",
    "Implement `__init__`, `__getitem__` and `__len__` 3 functions.  \n",
    "\n",
    "Some operations could be there: setting location of dataset, the method of reading data, label of dataset or transform of dataset.\n",
    "\n",
    "See [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details\n",
    "\n",
    "---\n",
    "\n",
    "繼承自定義資料集的框架 `torch.utils.data.Dataset`，主要實現 `__getitem__()` 和 `__len__()` 這兩個方法。\n",
    "\n",
    "常使用來做到設定資料位址、設定讀取方式、子資料集的標籤和轉換條件...等。\n",
    "\n",
    "See [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class FlowerData(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, mode='train', transform=None):\n",
    "        self.mode = mode # 'train', 'val' or 'test'\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(f'{data_folder}/{csv_file}', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.data_list.append(f\"{data_folder}/{row['file_path']}\")\n",
    "                if mode != 'test':\n",
    "                    self.labels.append(row['label'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = Image.open(self.data_list[index])\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.mode == 'test':\n",
    "            return data\n",
    "        label = int(self.labels[index])\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation \n",
    "\n",
    "Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "\n",
    "PyTorch use `torchvision.transforms` to do data augmentation.\n",
    "[You can see all function here.](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "**NOTICE**: There are some operations may not be necessary for predict, so we should write one for train and one for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# For TRAIN\n",
    "########################################################################\n",
    "#  TODO: use transforms.xxx method to do some data augmentation        #\n",
    "#  This one is for training, find the composition to get better result #\n",
    "########################################################################\n",
    "transforms_train = ...\n",
    "########################################################################\n",
    "#                           End of your code                           #\n",
    "########################################################################\n",
    "\n",
    "# For VAL, TEST\n",
    "########################################################################\n",
    "#  TODO: use transforms.xxx method to do some data augmentation        #\n",
    "#  This one is for validate and test,                                  #\n",
    "#  NOTICE some operation we usually not use in this part               #\n",
    "########################################################################\n",
    "transforms_test = ...\n",
    "########################################################################\n",
    "#                           End of your code                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate dataset\n",
    "\n",
    "Let's instantiate three `FlowerData` class.\n",
    "+ train_set: for labeled_training.\n",
    "+ unlabeled_set: for unlabeled_training.\n",
    "+ dataset_val: for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FlowerData('train_labeled_dataset.csv', mode='train', transform=transforms_train)\n",
    "unlabeled_set = FlowerData('train_unlabeled_dataset.csv', mode='test', transform=transforms_train)\n",
    "valid_set = FlowerData('val.csv', mode='val', transform=transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first image's shape in dataset_train :\", train_set[0][0].size())\n",
    "print(\"There are\", len(train_set), \"images in labeled_dataset_train.\")\n",
    "print(\"There are\", len(unlabeled_set), \"images in unlabeled_dataset_train.\")\n",
    "print(\"There are\", len(valid_set), \"images in dataset_val.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader`\n",
    "\n",
    "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
    "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
    "+ `batch_size` : how many samples per batch to load\n",
    "\n",
    "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#            You can adjust batch_size              #\n",
    "#####################################################\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! We have made all data prepared.  \n",
    "Let's go develop our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement CNN using PyTorch \n",
    "\n",
    "Try to use labeled data design and train a deep convolutional network from scratch to predict the class label of a flower image. \n",
    "\n",
    "**Again, the goal of this assignment is for you to test different convolutional structures. You cannot directly use the blocks/architectures of pre-trained models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.conv import Conv2d\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YourCNNModel(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        #     TODO: use nn.xxx method to generate a CNN model part             #\n",
    "        ########################################################################\n",
    "        self.conv = ... # You can change the variable name\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x): \n",
    "        if not isinstance(x, torch.Tensor):\n",
    "          x = torch.Tensor(x)\n",
    "        ########################################################################\n",
    "        #     TODO: forward your model and get output                          #\n",
    "        ########################################################################\n",
    "        out = ...\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# or\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YourCNNModel()\n",
    "model.to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made our model!  \n",
    "Next, PyTorch also provide many utility function(loss, optmizer...etc).  \n",
    "You can define them in one-line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "################################################################################\n",
    "# TODO: Define loss and optmizer functions                                     #\n",
    "# Try any loss or optimizer function and learning rate to get better result    #\n",
    "# hint: torch.nn and torch.optim                                               #\n",
    "################################################################################\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train function\n",
    "Let's define train function.  \n",
    "It will iterate input data 1 epoch and update model with optmizer.  \n",
    "Finally, calculate mean loss and total accuracy.\n",
    "\n",
    "Hint: [torch.max()](https://pytorch.org/docs/stable/generated/torch.max.html#torch-max) or [torch.argmax()](https://pytorch.org/docs/stable/generated/torch.argmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_data, model, criterion, optimizer):\n",
    "    '''\n",
    "    Argement:\n",
    "    input_data -- iterable data, typr torch.utils.data.Dataloader is prefer\n",
    "    model -- nn.Module, model contain forward to predict output\n",
    "    criterion -- loss function, used to evaluate goodness of model\n",
    "    optimizer -- optmizer function, method for weight updating\n",
    "    '''\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    for images, labels in input_data:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Forward, backward and optimize                                 #\n",
    "        # 1. zero the parameter gradients                                      #\n",
    "        # 2. process input through the network                                 #\n",
    "        # 3. compute the loss                                                  #\n",
    "        # 4. propagate gradients back into the network’s parameters            #\n",
    "        # 5. Update the weights of the network                                 #\n",
    "        ########################################################################\n",
    "        pass\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Get the counts of correctly classified images                  #\n",
    "        # 1. get the model predicted result                                    #\n",
    "        # 2. sum the number of this batch predicted images                     #\n",
    "        # 3. sum the number of correctly classified                            #\n",
    "        # 4. save this batch's loss into loss_list                             #\n",
    "        # dimension of outputs: [batch_size, number of classes]                #\n",
    "        # Hint 1: use outputs.data to get no auto_grad                         #\n",
    "        # Hint 2: use torch.max()                                              #\n",
    "        ########################################################################\n",
    "        _, predicted = ...\n",
    "        total_count += ...\n",
    "        acc_count += ...\n",
    "        loss_list ...\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "    # Compute this epoch accuracy and loss\n",
    "    acc = acc_count / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate function\n",
    "Next part is validate function.  \n",
    "It works as training function without optmizer and weight-updating part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(input_data, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in input_data:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            ####################################################################\n",
    "            # TODO: Get the predicted result and loss                          #\n",
    "            # 1. process input through the network                             #\n",
    "            # 2. compute the loss                                              #\n",
    "            # 3. get the model predicted result                                #\n",
    "            # 4. get the counts of correctly classified images                 #\n",
    "            # 5. save this batch's loss into loss_list                         #\n",
    "            ####################################################################\n",
    "            pass\n",
    "\n",
    "            total_count += ...\n",
    "            acc_count += ...\n",
    "            loss_list ...\n",
    "            ####################################################################\n",
    "            #                         End of your code                         #\n",
    "            ####################################################################\n",
    "\n",
    "    acc = acc_count / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in a loop\n",
    "Call train and test function in a loop.  \n",
    "Take a break and wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#     You can adjust those hyper parameters to loop for max_epochs times       #\n",
    "################################################################################\n",
    "max_epochs = 10\n",
    "log_interval = 2 # print acc and loss in per log_interval time\n",
    "\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_acc, train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc, val_loss = val(valid_loader, model, criterion)\n",
    "\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    if epoch % log_interval == 0:\n",
    "        print('=' * 20, 'Epoch', epoch, '=' * 20)\n",
    "        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(train_acc, train_loss))\n",
    "        print('  Val Acc: {:.6f}   Val Loss: {:.6f}'.format(val_acc, val_loss))\n",
    "\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_acc_list)), train_acc_list)\n",
    "plt.plot(range(len(val_acc_list)), val_acc_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your well-trained state_dict of model\n",
    "torch.save(model.state_dict(), 'NAME_OF_SUPERVISED_TRAINING_EXPERIMENT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finish training your classifier, next you should use this classifer to predict unlabel images with pseduo label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Use unlabeled data to enhance model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained classifier weights\n",
    "ckpt = torch.load('NAME_OF_SUPERVISED_TRAINING_EXPERIMENT.pt')\n",
    "model.load_state_dict(ckpt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unlabeled data set list, we will use it later\n",
    "unlabeled_set_list = []\n",
    "\n",
    "for img in unlabeled_set:\n",
    "    unlabeled_set_list.append(img)\n",
    "    \n",
    "print(len(unlabeled_set_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "################################################################################\n",
    "# TODO: Define loss and optmizer functions                                     #\n",
    "# Try any loss or optimizer function and learning rate to get better result    #\n",
    "# hint: torch.nn and torch.optim                                               #\n",
    "################################################################################\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained classifier to generates pseudo-labels of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "###########################################################\n",
    "#   You can adjust the threshold to get better result !   #                                  \n",
    "###########################################################\n",
    "def get_pseudo_labels(model, threshold=0.5):\n",
    "    \n",
    "    global unlabeled_set_list, train_set \n",
    "    \n",
    "    remove_index, index = [], 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # Iterate over the dataset by batches.\n",
    "    for img in tqdm(unlabeled_set_list):\n",
    "        \n",
    "        #####################################################################################\n",
    "        #     TODO:                                                                         #\n",
    "        #     1. Foward the data, Using torch.no_grad() accelerates the forward process     #\n",
    "        #     2. obtain the probability distributions by applying softmax on logits         #\n",
    "        #     3. Filter the data with threshold                                             #\n",
    "        #     4. Combine the labeled training data with the pseudo-labeled data             #\n",
    "        #        to construct a new training set. then removed                              #\n",
    "        #     5. the unlabeled data from unlabeled_set_list                                 #\n",
    "        #     hint: ConcatDataset                                                           #\n",
    "        ##################################################################################### \n",
    "        ...\n",
    "        #####################################################################################\n",
    "        #                           End of your code                                        #\n",
    "        #####################################################################################\n",
    "\n",
    "     \n",
    "    remove_index.reverse()\n",
    "    for i in remove_index:\n",
    "        del unlabeled_set_list[i]\n",
    "\n",
    "    print(f\"[{len(train_set)-843}/1713] images have been labeled.\")\n",
    "    \n",
    "    # # Turn off the eval mode.\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define train function.  \n",
    "\n",
    "Use the **get_pseudo_labels** function to get the new training set, then construct a new data loader for training.\n",
    "\n",
    "It will iterate input data 1 epoch and update model with optmizer.  \n",
    "\n",
    "Finally, calculate mean loss and total accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "#########################################################################################################\n",
    "#         You can adjust those hyper parameters like epochs or threshlod for training                   #\n",
    "#########################################################################################################\n",
    "n_epochs = 10\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    #########################################################################################################\n",
    "    #    TODO:                                                                                              #\n",
    "    #    In each epoch, relabel the unlabeled dataset for semi-supervised learning.                         #\n",
    "    #    1. Obtain pseudo-labels for unlabeled data using trained model.(use get_pseudo_labels function)    #\n",
    "    #    2. Construct a new dataset and a data loader for training.                                         #\n",
    "    #    You can try different way to use the get_pseudo_label function maybe will get the better result.   #                                  #\n",
    "    ######################################################################################################### \n",
    "    ...\n",
    "    #########################################################################################################  \n",
    "    #                                          End of your code                                             #\n",
    "    #########################################################################################################\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        imgs, labels = batch\n",
    "\n",
    "        logits = model(imgs.to(device))\n",
    "        \n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        imgs, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "          logits = model(imgs.to(device))\n",
    "\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accs.append(acc)\n",
    "\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "    valid_acc_last = valid_acc\n",
    "\n",
    "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "    \n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        torch.save(model.state_dict(), 'NAME_OF_SELF_TRAINING_EXPERIMENT.pt')\n",
    "        print('[{:03d}/{:03d}] saving model with acc {:.3f}'.format(epoch + 1, n_epochs, best_acc))\n",
    "#########################################################################################################\n",
    "#                               End of your code                                                        #\n",
    "#########################################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your well-trained state_dict of model\n",
    "torch.save(model.state_dict(), 'NAME_OF_SELF_TRAINING_EXPERIMENT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/e304bb12c8a84e5c9c1b27a6c3bd4026).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. Click the folder icon in the left hand side of Colab.\n",
    "2. Right click \"result.csv\". Select \"Download\"\n",
    "3. To kaggle. Click \"Submit Predictions\"\n",
    "4. Upload the result.csv\n",
    "5. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard.\n",
    "\n",
    "---\n",
    "\n",
    "預測`test`並將結果上傳至Kaggle。[**連結**](https://www.kaggle.com/t/e304bb12c8a84e5c9c1b27a6c3bd4026)\n",
    "\n",
    "執行完畢此區的程式碼後，會將`test`預測完的結果存下來。\n",
    "\n",
    "上傳流程\n",
    "1. 點選左側選單最下方的資料夾圖示\n",
    "2. 右鍵「result.csv」\n",
    "3. 點選「Download」\n",
    "4. 至連結網頁點選「Submit Predictions」\n",
    "5. 將剛剛下載的檔案上傳\n",
    "6. 系統會計算並公布其中50%資料的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wanna load previous best model\n",
    "ckpt = torch.load('NAME_OF_SELF_TRAINING_EXPERIMENT.pt')\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = FlowerData('test.csv', mode='test', transform=transforms_test)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, model):\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in input_data:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            output_list.extend(predicted.to('cpu').numpy().tolist())\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "output_csv = predict(test_loader, model)\n",
    "with open('result.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'label'])\n",
    "    writer.writeheader()\n",
    "    for result in output_csv:\n",
    "        file_path = test_set.data_list[idx].replace(data_folder + '/', '')\n",
    "        writer.writerow({'file_path':file_path, 'label':result})\n",
    "        idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ee1b965d6f75a20b2b6babb72920dce4fab5775c12eb1659af0fb55d185fed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
