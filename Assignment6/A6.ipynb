{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Z1Snuk7rIK"
      },
      "source": [
        "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwr9MgZogRZ"
      },
      "source": [
        "Before we start, please put your name and SID in following format: <br>\n",
        ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DzsjuDhlz_k"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hi I'm 池品叡, B094020030"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-Zzebq7rIM"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc9gd_Wk7rIN"
      },
      "source": [
        "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
        "\n",
        "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
        "\n",
        "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
        "\n",
        "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notice \n",
        "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
        "\n",
        "You can use BERT and RoBERTa encoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs",
        "tags": []
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.  \n",
        "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
        "This is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vuw-gNvjqcYe",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]\n",
            "torch 2.1.0+cu121\n",
            "torchvision 0.16.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Text Sentiment Classification (40 points)\n",
        "\n",
        "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4s_a5D7rIR"
      },
      "source": [
        "## Loading Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPUkTbnL7rIR"
      },
      "source": [
        "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
        "\n",
        "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rK0ouXa09pDU"
      },
      "outputs": [],
      "source": [
        "## you might need some additional installations there\n",
        "#%echo happy installation\n",
        "#%pip -V\n",
        "#%pip install grpcio\n",
        "#%pip install google-auth\n",
        "#%pip install protobuf==3.9.2\n",
        "#%pip install pyprind\n",
        "#%pip install tqdm boto3 requests regex sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dmGCAevi7rIS",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Asus/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "Using cache found in C:\\Users\\Asus/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.13MB/s]\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.14MB/s]\n",
            "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.09MB/s]\n",
            "config.json: 100%|██████████| 481/481 [00:00<00:00, 240kB/s]\n",
            "model.safetensors: 100%|██████████| 499M/499M [01:07<00:00, 7.44MB/s] \n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "#########################################################################\n",
        "#            Loading tokenizer and model from transformer               #\n",
        "#########################################################################\n",
        "# from transformers import xxx\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification \n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "\n",
        "# ---------- 1. load from torch.hub ----------\n",
        "tokenizer_hub = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model1 = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', bert_type)\n",
        "\n",
        "# ---------- 2. load from installed huggingface ----------\n",
        "roberta_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model2 = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "# finetune from the output from bert to your task\n",
        "model1.classifier = nn.Linear(768, 3, bias=True)\n",
        "model2.classifier.out_proj = nn.Linear(768, 3, bias=True)\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "## How to Get Data\n",
        "\n",
        "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
        "\n",
        "- `train.csv`, `test.csv` and `val.csv`\n",
        "\n",
        "Training set 有 **10248** 筆資料.  \n",
        "Validation set 有 **1317** 筆資料.  \n",
        "Testing set 有 **3075** 筆資料.  \n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wf5GXTme7rIT",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# Utility function to extract text and label from csv file\n",
        "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
        "    text_list = []\n",
        "    label_list = []\n",
        "\n",
        "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
        "    with open(f_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for line in reader:\n",
        "            text_list.append(line['text'])\n",
        "            if mode != 'test':\n",
        "                label_list.append(int(line['sentiment_label']))\n",
        "\n",
        "    return text_list, label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoader 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6fpY0ZrK7rIV",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TwitterDataset_1(Dataset):\n",
        "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
        "        self.mode = mode\n",
        "\n",
        "        text_list, label_list = get_texts(f_name, mode)\n",
        "        print('mode', mode, 'has', len(text_list), 'datas')\n",
        "        text_list = tokenizer_hub(text_list,\n",
        "                             truncation=True, padding=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "        self.text_list = text_list['input_ids']\n",
        "        self.mask_list = text_list['attention_mask']\n",
        "\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        mask = self.mask_list[idx]\n",
        "        if self.mode == 'test':\n",
        "            return text, mask\n",
        "        label = torch.tensor(self.label_list[idx])\n",
        "        return text, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoader 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TwitterDataset_2(Dataset):\n",
        "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
        "        self.mode = mode\n",
        "\n",
        "        text_list, label_list = get_texts(f_name, mode)\n",
        "        print('mode', mode, 'has', len(text_list), 'datas')\n",
        "        text_list = roberta_tokenizer(text_list,\n",
        "                             truncation=True, padding=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "        self.text_list = text_list['input_ids']\n",
        "        self.mask_list = text_list['attention_mask']\n",
        "\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        mask = self.mask_list[idx]\n",
        "        if self.mode == 'test':\n",
        "            return text, mask\n",
        "        label = torch.tensor(self.label_list[idx])\n",
        "        return text, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `DataLoader`\n",
        "\n",
        "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
        "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
        "+ `batch_size` : how many samples per batch to load\n",
        "\n",
        "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nCmM4FSw7rIW",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mode train has 10248 datas\n",
            "mode val has 1317 datas\n",
            "mode test has 3075 datas\n"
          ]
        }
      ],
      "source": [
        "dataset_train1 = TwitterDataset_1(mode='train')\n",
        "dataset_val1 = TwitterDataset_1(mode='val')\n",
        "dataset_test1 = TwitterDataset_1(mode='test')\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_data1 = DataLoader(dataset_train1, batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "val_data1 = DataLoader(dataset_val1, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n",
        "test_data1 = DataLoader(dataset_test1, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mode train has 10248 datas\n",
            "mode val has 1317 datas\n",
            "mode test has 3075 datas\n"
          ]
        }
      ],
      "source": [
        "dataset_train2 = TwitterDataset_2(mode='train')\n",
        "dataset_val2 = TwitterDataset_2(mode='val')\n",
        "dataset_test2 = TwitterDataset_2(mode='test')\n",
        "\n",
        "batch_size = 64\n",
        "train_data2 = DataLoader(dataset_train2, batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "val_data2 = DataLoader(dataset_val2, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n",
        "test_data2 = DataLoader(dataset_test2, batch_size=batch_size // 2,\n",
        "                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bqkvofHc7rIY",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token ['[CLS]', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "token to s [CLS] @ united i have never been mislead by a company as many times as i have this week by united airlines! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "token ['Ġlike', 'Ġlegal', 'Ġsend', 'Ġcreate', 'Ġchoice', 'Ġdrugs', 'ĠOhio', 'Ġobey', 'Ġchorus', 'ĠLee', 'ĠLike', 'Ġfootball', 'Ġstatus', 'Ġhousing', 'Ġcross', 'Ġdog', 'Ġhousing', 'Ġcreate', 'Ġchoice', '»', 'ĠiPhone', 'ĠLike', 'Ġsend', 'Why', 'Ġstar', 'a', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "token to s  like legal send create choice drugs Ohio obey chorus Lee Like football status housing cross dog housing create choice� iPhone Like sendWhy stara<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n"
          ]
        }
      ],
      "source": [
        "t1 = tokenizer_hub.convert_ids_to_tokens(dataset_train1[0][0]) # converts a sequence of numeric IDs in the training dataset into their corresponding tokens using the specified tokenizer.\n",
        "print('token', t1)\n",
        "print('token to s', tokenizer_hub.convert_tokens_to_string(t1)) # converts a sequence of tokens (t) back into the original text string using the specified tokenizer.\n",
        "t2 = roberta_tokenizer.convert_ids_to_tokens(dataset_train1[0][0]) # converts a sequence of numeric IDs in the training dataset into their corresponding tokens using the specified tokenizer.\n",
        "print('token', t2)\n",
        "print('token to s', roberta_tokenizer.convert_tokens_to_string(t2)) # converts a sequence of tokens (t) back into the original text string using the specified tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DxZrfCqW7rIY",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "optimizer1 = optim.AdamW(model1.parameters(), lr=1e-5) # AdamW optimizer(torch.optim version of the Adam algorithm with weight decay fix)\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "\n",
        "model1 = model1.to(device)\n",
        "criterion1 = criterion1.to(device)\n",
        "\n",
        "# Second model\n",
        "optimizer2 = optim.AdamW(model2.parameters(), lr=1e-5) # AdamW optimizer(torch.optim version of the Adam algorithm with weight decay fix)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "model2 = model2.to(device)\n",
        "criterion2 = criterion2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpwgE2Gd7rIZ"
      },
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zlaiAZAD7rIa",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "def accuracy(raw_preds, y):\n",
        "    preds = raw_preds.argmax(dim=1)\n",
        "    acc = (preds == y).sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dmc_Gms97rIa",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Testing process                              #\n",
        "        #########################################################################\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        # 2. Put correct variables into model\n",
        "        # 3. Get prediction\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(text, attention_mask=mask)\n",
        "\n",
        "        loss = criterion(outputs[0], label)\n",
        "        acc = accuracy(outputs[0], label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "def test(model, data, optimizer, criterion, log_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Training process                             #\n",
        "        #########################################################################\n",
        "        # 1. Put correct variables into model\n",
        "        # 2. Get prediction\n",
        "        # 3. Evalutate by criterion and accuracy\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(text, attention_mask=mask)\n",
        "\n",
        "        loss = criterion(outputs[0], label)\n",
        "        acc = accuracy(outputs[0], label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if log_loss:\n",
        "            val_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "# class for monitoring train and test acc/loss\n",
        "class Meter:\n",
        "    def __init__(self):\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.train_acc_list.append(train_acc)\n",
        "        self.val_loss_list.append(val_loss)\n",
        "        self.val_acc_list.append(val_acc)\n",
        "\n",
        "    def plot(self):\n",
        "        x = range(len(self.train_loss_list))\n",
        "        plt.plot(x, self.train_loss_list)\n",
        "        plt.plot(x, self.val_loss_list, color='r')\n",
        "        plt.legend(['train_loss', 'val_loss'])\n",
        "        plt.show()\n",
        "        plt.plot(x, self.train_acc_list)\n",
        "        plt.plot(x, self.val_acc_list, color='r')\n",
        "        plt.legend(['train_acc', 'val_acc'])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZyrKd57rIb"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Process 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bVDe-fRe7rIc",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [03:24<00:00,  1.27s/it]\n",
            "100%|██████████| 42/42 [00:11<00:00,  3.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 train_loss: 0.009572729331664422 train_acc: 0.7561475409836066\n",
            "Epoch 1 val_loss:  0.014075209482112615 val_acc : 0.8253606681852695\n",
            "---------- e 1 save best model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [05:38<00:00,  2.11s/it]\n",
            "100%|██████████| 42/42 [00:25<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 train_loss: 0.00630555148065602 train_acc: 0.8480679156908665\n",
            "Epoch 2 val_loss:  0.013590161897236412 val_acc : 0.8321943811693242\n",
            "---------- e 2 save best model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [04:35<00:00,  1.71s/it]\n",
            "100%|██████████| 42/42 [00:07<00:00,  5.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 train_loss: 0.005107709982379352 train_acc: 0.8784153005464481\n",
            "Epoch 3 val_loss:  0.013636648700195637 val_acc : 0.8344722854973424\n",
            "---------- e 3 save best model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [04:23<00:00,  1.63s/it]\n",
            "100%|██████████| 42/42 [00:07<00:00,  5.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 train_loss: 0.004133358333117324 train_acc: 0.9048594847775175\n",
            "Epoch 4 val_loss:  0.014676728463344892 val_acc : 0.8246013667425968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [04:20<00:00,  1.62s/it]\n",
            "100%|██████████| 42/42 [00:10<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 train_loss: 0.00319771382304826 train_acc: 0.9312060889929742\n",
            "Epoch 5 val_loss:  0.014807926244813108 val_acc : 0.8420652999240699\n",
            "---------- e 5 save best model ----------\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#                          Hyper-parameters                             #\n",
        "#########################################################################\n",
        "max_epoch = 5\n",
        "log_interval = 1\n",
        "best_acc = 0\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################\n",
        "\n",
        "m = Meter()\n",
        "\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    train_loss, train_acc = train(model1, train_data1, optimizer1, criterion1)\n",
        "    val_loss, val_acc = test(model1, val_data1, optimizer1, criterion1, log_loss=True)\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
        "            epoch, train_loss, train_acc\n",
        "        ))\n",
        "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
        "            epoch, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "    # model checkpoint\n",
        "    torch.save(model1.state_dict(), 'ckpts/bert_e{}.pt'.format(epoch))\n",
        "    if val_acc > best_acc:\n",
        "        best_model = model1\n",
        "        best_acc = val_acc\n",
        "        print('-'*10, 'e', epoch, 'save best model', '-'*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Process 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [1:32:28<00:00, 34.46s/it]\n",
            "100%|██████████| 42/42 [00:59<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 train_loss: 0.00969712593527998 train_acc: 0.7459016393442623\n",
            "Epoch 1 val_loss:  0.012518795833681783 val_acc : 0.8390280941533789\n",
            "---------- e 1 save best model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/161 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#                          Hyper-parameters                             #\n",
        "#########################################################################\n",
        "max_epoch = 5\n",
        "log_interval = 1\n",
        "best_acc = 0\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################\n",
        "\n",
        "m2 = Meter()\n",
        "\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    train_loss, train_acc = train(model2, train_data2, optimizer2, criterion2)\n",
        "    val_loss, val_acc = test(model2, val_data2, optimizer2, criterion2, log_loss=True)\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
        "            epoch, train_loss, train_acc\n",
        "        ))\n",
        "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
        "            epoch, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "    m2.update(train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "    # model checkpoint\n",
        "    torch.save(model2.state_dict(), 'ckpts/roberta_e{}.pt'.format(epoch))\n",
        "    if val_acc > best_acc:\n",
        "        best_model = model2\n",
        "        best_acc = val_acc\n",
        "        print('-'*10, 'e', epoch, 'save best model', '-'*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SmtW58OR7rIc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot them out\n",
        "m.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcJcHf7n7rId"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from json import load\n",
        "\n",
        "best_model = model1\n",
        "#best_model.load_state_dict(torch.load('ckpts/e5.pt'))\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Sf5UTlMZ7rId"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97/97 [00:02<00:00, 34.61it/s]\n"
          ]
        }
      ],
      "source": [
        "best_model.eval()\n",
        "\n",
        "total_out = []\n",
        "for text, mask in tqdm(test_data1, total=len(test_data1)):\n",
        "    text = text.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    output = best_model(text, mask)\n",
        "    pred = output.logits\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    total_out.append(pred)\n",
        "\n",
        "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(total_out):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: In-Context learning (32 points)\n",
        "\n",
        "In this task, you will learn how to perform sentiment classification using **prompts** without the need for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pyprind\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading model and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#         TODO: Design your own template(prefix) and verbalizer         #\n",
        "#########################################################################\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.prefix = (\n",
        "            \"Example 1: Sentence: 'So excited I was put on an earlier flight to get home! Woo Hoo! #travel 🎉🎉🎉' Sentiment: 'positive'\\n\"\n",
        "            \"Example 2: Sentence: 'Just signed up for TrueBlue and booked a flight but keep getting an error when I try to link it to my TB account. What's the deal?' Sentiment: 'neutral'.\\n\"\n",
        "            \"Example 3: Sentence: 'Filed a PIR with an agent at Indianapolis airport. Unable to pull up information online on your site. Have DMed details.' Sentiment: 'negative'\\n\"\n",
        "            \"Example 4: Sentence: 'Can't believe you have to pay money to be treated this poorly.' Sentiment: 'negative'\\n\"\n",
        "            \"Determine the sentiment of the following sentence: '[MASK]'\"\n",
        "        )\n",
        "        self.verbalizer = {\n",
        "            'negative': 0,\n",
        "            'neutral': 1,\n",
        "            'positive': 2,\n",
        "        }\n",
        "        \n",
        "        self.max_seq_length = 512\n",
        "        self.batch_size = 64\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "bert_config = BertConfig.from_pretrained(bert_type)\n",
        "\n",
        "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
        "\n",
        "#######################################################################\n",
        "#                        End of your code                             #\n",
        "#######################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtaion verbalizer ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# Utility function to obtaion verbalizer ids\n",
        "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
        "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
        "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
        "    return verbalizer_ids, index2ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concatenate original text and prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# Utility function to concatenate prefix and text\n",
        "def concatenate_prefix(texts, config):\n",
        "    ##################################################\n",
        "    #   TODO: concatenate your own prefix and text   #                               \n",
        "    ##################################################\n",
        "    prefix_texts = []\n",
        "    for text in texts:\n",
        "        prefix_texts.append(config.prefix + text)\n",
        "    ##################################################\n",
        "    #                 End of your code               #                               \n",
        "    ##################################################\n",
        "    return prefix_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "def load_data(config):\n",
        "    # ['texts', 'labels']\n",
        "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
        "    original_texts = df['text'].tolist()\n",
        "    labels = df['sentiment_label'].tolist()\n",
        "\n",
        "    texts = concatenate_prefix(original_texts, config)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "texts, labels = load_data(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# Batching of texts and labels for training or processing in batches\n",
        "def pack_batch(texts, labels, batch_size):\n",
        "    \"\"\"\n",
        "    :param texts: list\n",
        "    :param labels: list\n",
        "    :param batch_size: int\n",
        "    :return batch_X: list\n",
        "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
        "    :return batch_y: list\n",
        "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
        "    :return batch_count: int\n",
        "    \"\"\"\n",
        "    assert len(texts) == len(labels)\n",
        "\n",
        "    if len(texts) % batch_size != 0:\n",
        "        flag = False\n",
        "        batch_count = int(len(texts) / batch_size) + 1\n",
        "    else:\n",
        "        flag = True\n",
        "        batch_count = int(len(texts) / batch_size)\n",
        "\n",
        "    batch_X, batch_y = [], []\n",
        "\n",
        "    if flag:\n",
        "        for i in range(batch_count):\n",
        "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "    else:\n",
        "        for i in range(batch_count):\n",
        "            if i == batch_count - 1:\n",
        "                batch_X.append(texts[i * batch_size:])\n",
        "                batch_y.append(labels[i * batch_size:])\n",
        "            else:\n",
        "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "\n",
        "    return batch_X, batch_y, batch_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencing the model without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8699\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(verbalizer_ids[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[100 %] Time elapsed: 00:02:06 | ETA: 00:00:00"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.675351 | precision: 0.620959 | recall: 0.675351 | f1: 0.598181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 00:02:06\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    predict_all = np.array([], dtype=int)\n",
        "    labels_all = np.array([], dtype=int)\n",
        "    pper = pyprind.ProgPercent(batch_count)\n",
        "    for i in range(batch_count):\n",
        "        inputs = batch_X[i]\n",
        "        labels = batch_y[i]\n",
        "\n",
        "        # Using the BERT tokenizer (tokenizer.batch_encode_plus), adding special tokens, ensuring a maximum sequence length, and handling padding/truncation\n",
        "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
        "                                             max_length=config.max_seq_length,\n",
        "                                             padding='max_length', truncation=True)\n",
        "        \n",
        "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
        "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
        "\n",
        "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
        "        logits = bert(ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        # Find [MASK] logits\n",
        "        # shape: (batch_size, vocab_size)\n",
        "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
        "\n",
        "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
        "        # shape: (batch_size, verbalizer_size)\n",
        "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
        "\n",
        "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
        "        pseudo_distribution = softmax(verbalizer_logits)\n",
        "\n",
        "        #################################################################################\n",
        "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
        "        #   2. Convert the index to the corresponding word ID                           #\n",
        "        #   3. Convert the ID to a token                                                #\n",
        "        #   4. Find the label corresponding to the token                                #                                                                           \n",
        "        #################################################################################\n",
        "\n",
        "        pred_indices = pseudo_distribution.argmax(dim=1)\n",
        "\n",
        "        pred_ids = [verbalizer_ids[idx] for idx in pred_indices]\n",
        "\n",
        "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids) \n",
        "        \n",
        "        pred_labels = [config.verbalizer[token] for token in pred_tokens]\n",
        "\n",
        "        #################################################################################\n",
        "        #                             End of your code                                  #                                       \n",
        "        #################################################################################\n",
        "\n",
        "        predict_all = np.append(predict_all, pred_labels)\n",
        "        labels_all = np.append(labels_all, labels)\n",
        "\n",
        "        pper.update()\n",
        "    \n",
        "    acc = accuracy_score(labels_all, predict_all)\n",
        "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
        "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
        "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
        "\n",
        "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: LM-BFF (45 points)\n",
        "\n",
        "https://arxiv.org/pdf/2012.15723.pdf\n",
        "\n",
        "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install openprompt\n",
        "\n",
        "This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline.\n",
        "\n",
        "[OpenPrompt Documentation](https://thunlp.github.io/OpenPrompt/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openprompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import openprompt package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup cuda and whether to perform automatic generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cuda = True\n",
        "auto_t = True # Whether to perform automatic template generation\n",
        "auto_v = True # Whether to perform automatic verbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
        "dataset = {}\n",
        "dataset['train'] = SST2Processor().get_train_examples(\"/SST-2/\")\n",
        "dataset['validation'] = SST2Processor().get_dev_examples(\"/SST-2/\")\n",
        "dataset['test'] = SST2Processor().get_test_examples(\"/SST-2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print('load model...')\n",
        "from openprompt.plms import load_plm\n",
        "\n",
        "# load mlm model for main tasks\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
        "\n",
        "# load generation model for template generation\n",
        "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
        "\n",
        "# if you wish to do automatic label word generation, the verbalizer is not the final verbalizer, and is only used for template generation.\n",
        "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']]) # Manually generate the verbalizer\n",
        "\n",
        "\n",
        "###################################################################################################################\n",
        "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
        "#         compare auto generate template and manual generate template                                             #\n",
        "###################################################################################################################\n",
        "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
        "\n",
        "############################################\n",
        "#   LMBFFTemplateGenerationTemplate        #\n",
        "############################################\n",
        "import random\n",
        "\n",
        "# number of demonstrations\n",
        "num_demonstrations = 1  # try different number\n",
        "\n",
        "demonstrations = []\n",
        "\n",
        "for _ in range(num_demonstrations):\n",
        "    # random choice training set example with label 0 \n",
        "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
        "\n",
        "    # random choice training set example with label 1\n",
        "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
        "    \n",
        "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
        "    demonstrations.append(demonstration)\n",
        "\n",
        "# You can modify the demonstrations and try different combinations\n",
        "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
        "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "\n",
        "#############################################\n",
        "#   End of LMBFFTemplateGenerationTemplate  #\n",
        "#############################################\n",
        "\n",
        "########################################\n",
        "#          ManualTemplate              #\n",
        "########################################\n",
        "\n",
        "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
        "\n",
        "########################################\n",
        "#          End of ManualTemplate       # \n",
        "########################################\n",
        "\n",
        "###################################################################################################################\n",
        "#                                           End of your code                                                      #\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "# view wrapped example\n",
        "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
        "print(\"dataset:\", dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "\n",
        "# Returns the best evaluation score achieved during training\n",
        "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n",
        "    best_score = 0.0\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
        "        score = evaluate(model, val_dataloader)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
        "    return best_score\n",
        "\n",
        "# Trains the model on the training data and computes the training loss\n",
        "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        #####################################################\n",
        "        # 1. Put correct variables into model to get logits #\n",
        "        # 2. Get labels                                     #\n",
        "        # 3. Evalutate using loss_func                         #          \n",
        "        # 4. Append loss to loss_all                        #\n",
        "        #####################################################\n",
        "        logits = ...\n",
        "        labels = ...\n",
        "        loss = ...\n",
        "        loss.backward()\n",
        "        loss_all.append(...)\n",
        "        #####################################################\n",
        "        #                 End of your code                  #\n",
        "        #####################################################\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    with torch.no_grad():\n",
        "        for step, inputs in enumerate(val_dataloader):\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            #####################################################\n",
        "            # 1. Put correct variables into model to get logits #\n",
        "            # 2. Get labels                                     #\n",
        "            # 3. Extend labels to list                          #\n",
        "            # 4. Get predictions and extend preds to list        #\n",
        "            #####################################################\n",
        "            logits = ...\n",
        "            labels = ...\n",
        "            alllabels.extend(...)\n",
        "            allpreds.extend(...)\n",
        "            #####################################################\n",
        "            #                 End of your code                  #\n",
        "            #####################################################\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic template generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generated template from TemplateGenerator and find the best template "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class ManualTemplateWithoutParse(ManualTemplate):\n",
        "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
        "    def on_text_set(self):\n",
        "        pass\n",
        "\n",
        "# Template generation\n",
        "if auto_t:\n",
        "    print('performing auto_t...')\n",
        "\n",
        "    if cuda:\n",
        "        template_generate_model = template_generate_model.cuda()\n",
        "\n",
        "    # Creates an instance of T5TemplateGenerator, used for generating text templates\n",
        "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
        "\n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        template_generator._register_buffer(data)\n",
        "\n",
        "    template_generate_model.eval()\n",
        "    print('generating...')\n",
        "    template_texts = template_generator._get_templates() # Calls _get_templates on template_generator to generate template texts.\n",
        "\n",
        "    # Converting and Printing Templates\n",
        "    original_template = template.text\n",
        "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts] \n",
        "    # template_generator._show_template()\n",
        "    template_generator.release_memory()\n",
        "    # Generate a number of candidate template text\n",
        "    print(template_texts)\n",
        "    \n",
        "    # Iterate over each candidate and select the best one\n",
        "    best_metrics = 0.0\n",
        "    best_template_text = None\n",
        "    for template_text in tqdm(template_texts):\n",
        "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "        print(f\"current template: {template_text}, wrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
        "\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "        \n",
        "        #######################################################\n",
        "        # TODO: Use score to Find your best template_text     #\n",
        "        #######################################################\n",
        "        ...\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # Use the best template\n",
        "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "    print(\"final best template:\", best_template_text)\n",
        "    print(\"wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic erbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verbalizer template from VerbalizerGenerator and find the best verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verbalizer generation\n",
        "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
        "if auto_v:\n",
        "    print('performing auto_v...')\n",
        "    # Load generation model for verbalizer generation\n",
        "    if cuda:\n",
        "        plm = plm.cuda()\n",
        "\n",
        "    # Creates an instance of RobertaVerbalizerGenerator, used for generating verbalizer.\n",
        "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20) # To improve performance, try larger numbers\n",
        "    \n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        verbalizer_generator.register_buffer(data)\n",
        "\n",
        "    # Calls generate on verbalizer_generator to generate label words.\n",
        "    label_words_list = verbalizer_generator.generate()\n",
        "    verbalizer_generator.release_memory()\n",
        "\n",
        "    # Iterate over each candidate and select the best one\n",
        "    current_verbalizer = copy.deepcopy(verbalizer)\n",
        "    best_metrics = 0.0\n",
        "    best_label_words = None\n",
        "    for label_words in tqdm(label_words_list):\n",
        "        current_verbalizer.label_words = label_words\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "\n",
        "        #######################################################\n",
        "        # TODO: Use score to find your best_label_word        #\n",
        "        #######################################################\n",
        "        ...\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # use the best verbalizer\n",
        "    print(\"final best label words:\", best_label_words)\n",
        "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "\n",
        "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "allpreds = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = model(inputs)\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(allpreds):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Report (15 points)\n",
        "\n",
        "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
        "\n",
        "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
        "\n",
        "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "51ee1b965d6f75a20b2b6babb72920dce4fab5775c12eb1659af0fb55d185fed"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
