{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Z1Snuk7rIK"
      },
      "source": [
        "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwr9MgZogRZ"
      },
      "source": [
        "Before we start, please put your name and SID in following format: <br>\n",
        ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DzsjuDhlz_k"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hi I'm XXX, XXXXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-Zzebq7rIM"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc9gd_Wk7rIN"
      },
      "source": [
        "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
        "\n",
        "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
        "\n",
        "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
        "\n",
        "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notice \n",
        "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
        "\n",
        "You can use BERT and RoBERTa encoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs",
        "tags": []
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.  \n",
        "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
        "This is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vuw-gNvjqcYe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]\n",
            "torch 2.1.0+cu121\n",
            "torchvision 0.16.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)\n",
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Text Sentiment Classification (40 points)\n",
        "\n",
        "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4s_a5D7rIR"
      },
      "source": [
        "## Loading Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPUkTbnL7rIR"
      },
      "source": [
        "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
        "\n",
        "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK0ouXa09pDU"
      },
      "outputs": [],
      "source": [
        "# you might need some additional installations there\n",
        "!echo happy installation\n",
        "!pip -V\n",
        "!pip install grpcio\n",
        "!pip install google-auth\n",
        "!pip install protobuf==3.9.2\n",
        "!pip install pyprind\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dmGCAevi7rIS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Chi/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "Using cache found in C:\\Users\\Chi/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "#########################################################################\n",
        "#            Loading tokenizer and model from transformer               #\n",
        "#########################################################################\n",
        "# from transformers import xxx\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "\n",
        "# ---------- 1. load from torch.hub ----------\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', bert_type)\n",
        "\n",
        "# ---------- 2. load from installed huggingface ----------\n",
        "#tokenizer = XXXTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "## create a Bert-extended task (classification)\n",
        "#model = XXXForSequenceClassification.from_pretrained(bert_type)\n",
        "\n",
        "\n",
        "\n",
        "# finetune from the output from bert to your task\n",
        "model.classifier = nn.Linear(768, 3, bias=True)\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "## How to Get Data\n",
        "\n",
        "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZnFgi5i_2oA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
        "\n",
        "- `train.csv`, `test.csv` and `val.csv`\n",
        "\n",
        "Training set 有 **10248** 筆資料.  \n",
        "Validation set 有 **1317** 筆資料.  \n",
        "Testing set 有 **3075** 筆資料.  \n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSlTMdxf8Zd7"
      },
      "outputs": [],
      "source": [
        "!unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wf5GXTme7rIT"
      },
      "outputs": [],
      "source": [
        "# Utility function to extract text and label from csv file\n",
        "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
        "    text_list = []\n",
        "    label_list = []\n",
        "\n",
        "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
        "    with open(f_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for line in reader:\n",
        "            text_list.append(line['text'])\n",
        "            if mode != 'test':\n",
        "                label_list.append(int(line['sentiment_label']))\n",
        "\n",
        "    return text_list, label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6fpY0ZrK7rIV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
        "        self.mode = mode\n",
        "\n",
        "        text_list, label_list = get_texts(f_name, mode)\n",
        "        print('mode', mode, 'has', len(text_list), 'datas')\n",
        "        text_list = tokenizer(text_list,\n",
        "                             truncation=True, padding=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "        self.text_list = text_list['input_ids']\n",
        "        self.mask_list = text_list['attention_mask']\n",
        "\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        mask = self.mask_list[idx]\n",
        "        if self.mode == 'test':\n",
        "            return text, mask\n",
        "        label = torch.tensor(self.label_list[idx])\n",
        "        return text, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `DataLoader`\n",
        "\n",
        "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
        "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
        "+ `batch_size` : how many samples per batch to load\n",
        "\n",
        "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nCmM4FSw7rIW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mode train has 10248 datas\n",
            "mode val has 1317 datas\n",
            "mode test has 3075 datas\n"
          ]
        }
      ],
      "source": [
        "dataset_train = TwitterDataset(mode='train')\n",
        "dataset_val = TwitterDataset(mode='val')\n",
        "dataset_test = TwitterDataset(mode='test')\n",
        "\n",
        "batch_size = 64\n",
        "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n",
        "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
        "                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bqkvofHc7rIY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token ['[CLS]', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "token to s [CLS] @ united i have never been mislead by a company as many times as i have this week by united airlines! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0]) # converts a sequence of numeric IDs in the training dataset into their corresponding tokens using the specified tokenizer.\n",
        "print('token', t)\n",
        "print('token to s', tokenizer.convert_tokens_to_string(t)) # converts a sequence of tokens (t) back into the original text string using the specified tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DxZrfCqW7rIY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Chi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "from torch import nn\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpwgE2Gd7rIZ"
      },
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zlaiAZAD7rIa"
      },
      "outputs": [],
      "source": [
        "def accuracy(raw_preds, y):\n",
        "    preds = raw_preds.argmax(dim=1)\n",
        "    acc = (preds == y).sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dmc_Gms97rIa"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Testing process                              #\n",
        "        #########################################################################\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        # 2. Put correct variables into model\n",
        "        # 3. Get prediction\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "def test(model, data, criterion, log_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Training process                             #\n",
        "        #########################################################################\n",
        "        # 1. Put correct variables into model\n",
        "        # 2. Get prediction\n",
        "        # 3. Evalutate by criterion and accuracy\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if log_loss:\n",
        "            val_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "# class for monitoring train and test acc/loss\n",
        "class Meter:\n",
        "    def __init__(self):\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.train_acc_list.append(train_acc)\n",
        "        self.val_loss_list.append(val_loss)\n",
        "        self.val_acc_list.append(val_acc)\n",
        "\n",
        "    def plot(self):\n",
        "        x = range(len(self.train_loss_list))\n",
        "        plt.plot(x, self.train_loss_list)\n",
        "        plt.plot(x, self.val_loss_list, color='r')\n",
        "        plt.legend(['train_loss', 'val_loss'])\n",
        "        plt.show()\n",
        "        plt.plot(x, self.train_acc_list)\n",
        "        plt.plot(x, self.val_acc_list, color='r')\n",
        "        plt.legend(['train_acc', 'val_acc'])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZyrKd57rIb"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVDe-fRe7rIc"
      },
      "outputs": [],
      "source": [
        "#########################################################################\n",
        "#                          Hyper-parameters                             #\n",
        "#########################################################################\n",
        "max_epoch = 5\n",
        "log_interval = 1\n",
        "best_acc = 0\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################\n",
        "\n",
        "m = Meter()\n",
        "\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
        "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
        "            epoch, train_loss, train_acc\n",
        "        ))\n",
        "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
        "            epoch, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "    # model checkpoint\n",
        "    torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))\n",
        "    if val_acc > best_acc:\n",
        "        best_model = model\n",
        "        best_acc = val_acc\n",
        "        print('-'*10, 'e', epoch, 'save best model', '-'*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmtW58OR7rIc"
      },
      "outputs": [],
      "source": [
        "# plot them out\n",
        "m.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcJcHf7n7rId"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf5UTlMZ7rId"
      },
      "outputs": [],
      "source": [
        "best_model.eval()\n",
        "\n",
        "total_out = []\n",
        "for text, mask in tqdm(test_data, total=len(test_data)):\n",
        "    text = text.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    output = best_model(text, mask)\n",
        "    pred = output.logits\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    total_out.append(pred)\n",
        "\n",
        "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(total_out):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: In-Context learning (32 points)\n",
        "\n",
        "In this task, you will learn how to perform sentiment classification using **prompts** without the need for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pyprind\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading model and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#         TODO: Design your own template(prefix) and verbalizer         #\n",
        "#########################################################################\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.prefix = \"Determine the sentiment of the following sentence: [MASK]''\"\n",
        "        \n",
        "        self.verbalizer = {\n",
        "            'negative': 0,\n",
        "            'neutral': 1,\n",
        "            'positive': 2,\n",
        "        }\n",
        "        \n",
        "        self.max_seq_length = 512\n",
        "        self.batch_size = 64\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "bert_config = BertConfig.from_pretrained(bert_type)\n",
        "\n",
        "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
        "\n",
        "#######################################################################\n",
        "#                        End of your code                             #\n",
        "#######################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtaion verbalizer ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility function to obtaion verbalizer ids\n",
        "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
        "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
        "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
        "    return verbalizer_ids, index2ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concatenate original text and prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility function to concatenate prefix and text\n",
        "def concatenate_prefix(texts, config):\n",
        "    ##################################################\n",
        "    #   TODO: concatenate your own prefix and text   #                               \n",
        "    ##################################################\n",
        "    prefix_texts = []\n",
        "    for text in texts:\n",
        "        prefix_texts.append(config.prefix + text)\n",
        "    ##################################################\n",
        "    #                 End of your code               #                               \n",
        "    ##################################################\n",
        "    return prefix_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(config):\n",
        "    # ['texts', 'labels']\n",
        "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
        "    original_texts = df['text'].tolist()\n",
        "    labels = df['sentiment_label'].tolist()\n",
        "\n",
        "    texts = concatenate_prefix(original_texts, config)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "texts, labels = load_data(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batching of texts and labels for training or processing in batches\n",
        "def pack_batch(texts, labels, batch_size):\n",
        "    \"\"\"\n",
        "    :param texts: list\n",
        "    :param labels: list\n",
        "    :param batch_size: int\n",
        "    :return batch_X: list\n",
        "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
        "    :return batch_y: list\n",
        "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
        "    :return batch_count: int\n",
        "    \"\"\"\n",
        "    assert len(texts) == len(labels)\n",
        "\n",
        "    if len(texts) % batch_size != 0:\n",
        "        flag = False\n",
        "        batch_count = int(len(texts) / batch_size) + 1\n",
        "    else:\n",
        "        flag = True\n",
        "        batch_count = int(len(texts) / batch_size)\n",
        "\n",
        "    batch_X, batch_y = [], []\n",
        "\n",
        "    if flag:\n",
        "        for i in range(batch_count):\n",
        "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "    else:\n",
        "        for i in range(batch_count):\n",
        "            if i == batch_count - 1:\n",
        "                batch_X.append(texts[i * batch_size:])\n",
        "                batch_y.append(labels[i * batch_size:])\n",
        "            else:\n",
        "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "\n",
        "    return batch_X, batch_y, batch_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencing the model without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[100 %] Time elapsed: 00:02:42 | ETA: 00:00:00"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.212139 | precision: 0.600359 | recall: 0.212139 | f1: 0.146791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 00:02:42\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    predict_all = np.array([], dtype=int)\n",
        "    labels_all = np.array([], dtype=int)\n",
        "    pper = pyprind.ProgPercent(batch_count)\n",
        "    for i in range(batch_count):\n",
        "        inputs = batch_X[i]\n",
        "        labels = batch_y[i]\n",
        "\n",
        "        # Using the BERT tokenizer (tokenizer.batch_encode_plus), adding special tokens, ensuring a maximum sequence length, and handling padding/truncation\n",
        "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
        "                                             max_length=config.max_seq_length,\n",
        "                                             padding='max_length', truncation=True)\n",
        "        \n",
        "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
        "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
        "\n",
        "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
        "        logits = bert(ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        # Find [MASK] logits\n",
        "        # shape: (batch_size, vocab_size)\n",
        "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
        "\n",
        "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
        "        # shape: (batch_size, verbalizer_size)\n",
        "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
        "\n",
        "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
        "        pseudo_distribution = softmax(verbalizer_logits)\n",
        "\n",
        "        #################################################################################\n",
        "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
        "        #   2. Convert the index to the corresponding word ID                           #\n",
        "        #   3. Convert the ID to a token                                                #\n",
        "        #   4. Find the label corresponding to the token                                #                                                                           \n",
        "        #################################################################################\n",
        "\n",
        "        pred_indices = pseudo_distribution.argmax(dim=1)\n",
        "\n",
        "        pred_ids = [verbalizer_ids[idx] for idx in pred_indices]\n",
        "        \n",
        "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids) \n",
        "        \n",
        "        pred_labels = [config.verbalizer[token] for token in pred_tokens]\n",
        "\n",
        "        #################################################################################\n",
        "        #                             End of your code                                  #                                       \n",
        "        #################################################################################\n",
        "\n",
        "        predict_all = np.append(predict_all, pred_labels)\n",
        "        labels_all = np.append(labels_all, labels)\n",
        "\n",
        "        pper.update()\n",
        "\n",
        "    acc = accuracy_score(labels_all, predict_all)\n",
        "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
        "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
        "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
        "\n",
        "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: LM-BFF (45 points)\n",
        "\n",
        "https://arxiv.org/pdf/2012.15723.pdf\n",
        "\n",
        "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install openprompt\n",
        "\n",
        "This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline.\n",
        "\n",
        "[OpenPrompt Documentation](https://thunlp.github.io/OpenPrompt/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openprompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import openprompt package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup cuda and whether to perform automatic generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cuda = True\n",
        "auto_t = True # Whether to perform automatic template generation\n",
        "auto_v = True # Whether to perform automatic verbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
        "dataset = {}\n",
        "dataset['train'] = SST2Processor().get_train_examples(\"/SST-2/\")\n",
        "dataset['validation'] = SST2Processor().get_dev_examples(\"/SST-2/\")\n",
        "dataset['test'] = SST2Processor().get_test_examples(\"/SST-2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print('load model...')\n",
        "from openprompt.plms import load_plm\n",
        "\n",
        "# load mlm model for main tasks\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
        "\n",
        "# load generation model for template generation\n",
        "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
        "\n",
        "# if you wish to do automatic label word generation, the verbalizer is not the final verbalizer, and is only used for template generation.\n",
        "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']]) # Manually generate the verbalizer\n",
        "\n",
        "\n",
        "###################################################################################################################\n",
        "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
        "#         compare auto generate template and manual generate template                                             #\n",
        "###################################################################################################################\n",
        "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
        "\n",
        "############################################\n",
        "#   LMBFFTemplateGenerationTemplate        #\n",
        "############################################\n",
        "import random\n",
        "\n",
        "# number of demonstrations\n",
        "num_demonstrations = 1  # try different number\n",
        "\n",
        "demonstrations = []\n",
        "\n",
        "for _ in range(num_demonstrations):\n",
        "    # random choice training set example with label 0 \n",
        "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
        "\n",
        "    # random choice training set example with label 1\n",
        "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
        "    \n",
        "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
        "    demonstrations.append(demonstration)\n",
        "\n",
        "# You can modify the demonstrations and try different combinations\n",
        "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
        "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "\n",
        "#############################################\n",
        "#   End of LMBFFTemplateGenerationTemplate  #\n",
        "#############################################\n",
        "\n",
        "########################################\n",
        "#          ManualTemplate              #\n",
        "########################################\n",
        "\n",
        "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
        "\n",
        "########################################\n",
        "#          End of ManualTemplate       # \n",
        "########################################\n",
        "\n",
        "###################################################################################################################\n",
        "#                                           End of your code                                                      #\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "# view wrapped example\n",
        "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
        "print(\"dataset:\", dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "\n",
        "# Returns the best evaluation score achieved during training\n",
        "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n",
        "    best_score = 0.0\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
        "        score = evaluate(model, val_dataloader)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
        "    return best_score\n",
        "\n",
        "# Trains the model on the training data and computes the training loss\n",
        "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        #####################################################\n",
        "        # 1. Put correct variables into model to get logits #\n",
        "        # 2. Get labels                                     #\n",
        "        # 3. Evalutate using loss_func                         #          \n",
        "        # 4. Append loss to loss_all                        #\n",
        "        #####################################################\n",
        "        logits = ...\n",
        "        labels = ...\n",
        "        loss = ...\n",
        "        loss.backward()\n",
        "        loss_all.append(...)\n",
        "        #####################################################\n",
        "        #                 End of your code                  #\n",
        "        #####################################################\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    with torch.no_grad():\n",
        "        for step, inputs in enumerate(val_dataloader):\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            #####################################################\n",
        "            # 1. Put correct variables into model to get logits #\n",
        "            # 2. Get labels                                     #\n",
        "            # 3. Extend labels to list                          #\n",
        "            # 4. Get predictions and extend preds to list        #\n",
        "            #####################################################\n",
        "            logits = ...\n",
        "            labels = ...\n",
        "            alllabels.extend(...)\n",
        "            allpreds.extend(...)\n",
        "            #####################################################\n",
        "            #                 End of your code                  #\n",
        "            #####################################################\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic template generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generated template from TemplateGenerator and find the best template "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class ManualTemplateWithoutParse(ManualTemplate):\n",
        "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
        "    def on_text_set(self):\n",
        "        pass\n",
        "\n",
        "# Template generation\n",
        "if auto_t:\n",
        "    print('performing auto_t...')\n",
        "\n",
        "    if cuda:\n",
        "        template_generate_model = template_generate_model.cuda()\n",
        "\n",
        "    # Creates an instance of T5TemplateGenerator, used for generating text templates\n",
        "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
        "\n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        template_generator._register_buffer(data)\n",
        "\n",
        "    template_generate_model.eval()\n",
        "    print('generating...')\n",
        "    template_texts = template_generator._get_templates() # Calls _get_templates on template_generator to generate template texts.\n",
        "\n",
        "    # Converting and Printing Templates\n",
        "    original_template = template.text\n",
        "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts] \n",
        "    # template_generator._show_template()\n",
        "    template_generator.release_memory()\n",
        "    # Generate a number of candidate template text\n",
        "    print(template_texts)\n",
        "    \n",
        "    # Iterate over each candidate and select the best one\n",
        "    best_metrics = 0.0\n",
        "    best_template_text = None\n",
        "    for template_text in tqdm(template_texts):\n",
        "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "        print(f\"current template: {template_text}, wrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
        "\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "        \n",
        "        #######################################################\n",
        "        # TODO: Use score to Find your best template_text     #\n",
        "        #######################################################\n",
        "        ...\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # Use the best template\n",
        "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "    print(\"final best template:\", best_template_text)\n",
        "    print(\"wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic erbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verbalizer template from VerbalizerGenerator and find the best verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verbalizer generation\n",
        "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
        "if auto_v:\n",
        "    print('performing auto_v...')\n",
        "    # Load generation model for verbalizer generation\n",
        "    if cuda:\n",
        "        plm = plm.cuda()\n",
        "\n",
        "    # Creates an instance of RobertaVerbalizerGenerator, used for generating verbalizer.\n",
        "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20) # To improve performance, try larger numbers\n",
        "    \n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        verbalizer_generator.register_buffer(data)\n",
        "\n",
        "    # Calls generate on verbalizer_generator to generate label words.\n",
        "    label_words_list = verbalizer_generator.generate()\n",
        "    verbalizer_generator.release_memory()\n",
        "\n",
        "    # Iterate over each candidate and select the best one\n",
        "    current_verbalizer = copy.deepcopy(verbalizer)\n",
        "    best_metrics = 0.0\n",
        "    best_label_words = None\n",
        "    for label_words in tqdm(label_words_list):\n",
        "        current_verbalizer.label_words = label_words\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "\n",
        "        #######################################################\n",
        "        # TODO: Use score to find your best_label_word        #\n",
        "        #######################################################\n",
        "        ...\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # use the best verbalizer\n",
        "    print(\"final best label words:\", best_label_words)\n",
        "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "\n",
        "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "allpreds = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = model(inputs)\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(allpreds):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Report (15 points)\n",
        "\n",
        "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
        "\n",
        "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
        "\n",
        "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "51ee1b965d6f75a20b2b6babb72920dce4fab5775c12eb1659af0fb55d185fed"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
